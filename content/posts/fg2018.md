---
title: "IEEE FG 2018 Workshop on Dense 3D Reconstruction of 2D Face Images in the Wild"
slug: "fg2018"
feature_image: ""
date: "2017-10-12T15:14:20.000Z"
layout: "page"
people:
  - Zhenhua Feng
tags:
  - Events
---

<div class="kg-card-markdown"><h3 id="news">News</h3>
<ul>
<li>01-FEB-2018: The evaluation script of the competition is available via <a href="https://github.com/patrikhuber/fg2018-competition">https://github.com/patrikhuber/fg2018-competition</a>. You have to use the script and the provided 7 3D facial landmarks for evaluation. You also should send us back the average error of the reconstructed 3D face for each 2D image.</li>
<li>31-JAN-2018: The paper submission deadline has been extended to <strong>07-02-2018</strong>.</li>
<li>01-JAN-2018: Happy New Year! The test set for the workshop special session, i.e. the competition, is ready to download. The test set has 2000 2D images, including 656 high-quality and 1344 low-quality images, of more than one hundred subjects. The test set is a subset of the <a href="http://pics.stir.ac.uk/ESRC/index.htm">Stirling/ESRC 3D Face Database</a>. To obtain the test set, please follow the instruction of the database webpage. You have to download the <a href="http://pics.stir.ac.uk/ESRC/license%20agreement.pdf">license agreement</a>, sign and scan it and return to <a href="mailto:3dfacedb@gmail.com">3dfacedb@gmail.com</a> (and cc to <a href="mailto:z.feng@surrey.ac.uk">z.feng@surrey.ac.uk</a> please). Please note that any 2D or 3D face image/scan of the Stirling/ESRC 3D Dace Database shall not be used for your model training or hyper-parameter tuning. You have to use the provided validation set for this kind of purpose. The evaluation metric will be announced very soon.</li>
<li>05-DEC-2017: The validation set for the challenge is ready. To obtain the validation set, please download, print and sign the agreement file (<a href="http://personal.ee.surrey.ac.uk/Personal/Z.Feng/files/agreement.pdf">link</a>). Then you should send a scanned copy of the signed agreement file to Mr. Hefeng Yin (Email: <a href="mailto:yinhefeng@126.com">yinhefeng@126.com</a>, <a href="mailto:7141905017@vip.jiangnan.edu.cn">7141905017@vip.jiangnan.edu.cn</a>) to obtain the download link. The validation set has 161 2D in-the-wild images of 10 subjects and their ground-truth 3D face scans. The validation set is a part of the JNU-3D face dataset collected by the Jiangnan University. Please note that the 2D images and 3D face scans of the test set are in different format and resolution than the validation set. The test set and evaluation protocol will be released at the beginning of January 2018.</li>
<li>27-NOV-2017: Paper submission is open. Please go to the <strong><a href="https://cmt3.research.microsoft.com/DRFIW2018">CMT</a></strong> website for submission.</li>
<li>12-OCT-2017: The workshop webpage is online.</li>
</ul>
<h3 id="topicsofinterest">Topics of Interest</h3>
<p>3D face reconstruction from 2D images has become a very active topic in computer vision and computer graphics. The workshop aims to bring together the community to explore and address challenges in 3D face reconstruction of 2D in-the-wild images. Topics of interest include but are not limited to:</p>
<ul>
<li>3D face dataset and models</li>
<li>3D face reconstruction from single images and videos</li>
<li>3D-based face analysis and biometrics</li>
<li>Special session on 3D face reconstruction challenge (competition)</li>
<li>Other applications of 3D face models</li>
</ul>
<p>The workshop is co-located with the <a href="https://fg2018.cse.sc.edu/">13th IEEE Conference on Automatic Face &amp; Gesture Recognition</a> (FG2018), 15-19 May, 2018, Xian,China.</p>
<p><em><strong>Special Session</strong>: Competition</em><br>
As a special session of the workshop, we will release a new benchmark dataset consisting of a number of identities.  For each identity there are a number of 2D in-the-wild face images and a high-resolution 3D face scan ground-truth. Alongside  the dataset we supply a standard protocol to allow independent comparison among different algorithms.<br>
The submission deadline for all papers is <strong>07 FEB 2018</strong>.</p>
<h3 id="papersubmissionsystem">Paper Submission System</h3>
<p>The paper submission system is open. To prepare your manuscript, please follow the guideline of the IEEE FG 2018 main conference (<a href="https://fg2018.cse.sc.edu/submissions.html">link</a>). Note that, for the workshop, you have to submit your manuscript via the <strong><a href="https://cmt3.research.microsoft.com/DRFIW2018">CMT</a></strong> system, which is different from the submission system of the main FG conference.</p>
<h3 id="importantdates">Important Dates</h3>
<ul>
<li>01-12-2017 - Release of the Validation Set and Protocol (Competition)</li>
<li>01-01-2018 - Release of the Test Set (Competition)</li>
<li>07-02-2018 - Paper Submission Deadline</li>
<li>21-02-2018 - Decisions to Authors</li>
<li>01-03-2018 - Camera-Ready Deadline</li>
<li>15/19-05-2018 - Workshop</li>
</ul>
<h3 id="organisers">Organisers</h3>
<ul>
<li>Dr. <a href="https://sites.google.com/view/fengzhenhua">Zhenhua Feng</a>, University of Surrey, UK (<a href="mailto:z.feng@surrey.ac.uk">z.feng@surrey.ac.uk</a>)</li>
<li>Dr. <a href="http://www.patrikhuber.ch/">Patrik Huber</a>, University of Surrey, UK (<a href="mailto:p.huber@surrey.ac.uk">p.huber@surrey.ac.uk</a>)</li>
<li>Prof. <a href="https://www.stir.ac.uk/people/11587">Peter Hancock</a>, University of Stirling, UK (<a href="mailto:p.j.b.hancock@stir.ac.uk">p.j.b.hancock@stir.ac.uk</a>)</li>
<li>Prof. <a href="https://www.surrey.ac.uk/cvssp/people/josef_kittler/">Josef Kittler</a>, University of Surrey, UK (<a href="mailto:j.kittler@surrey.ac.uk">j.kittler@surrey.ac.uk</a>)</li>
<li>Dr. <a href="https://paulkoppen.com/">Paul Koppen</a>, University of Surrey, UK (<a href="mailto:p.koppen@surrey.ac.uk">p.koppen@surrey.ac.uk</a>)</li>
<li>Prof. <a href="https://scholar.google.com/citations?user=5IST34sAAAAJ&amp;hl=en">Xiaojun Wu</a>, Jiangnan University, China (<a href="mailto:wu_xiaojun@jiangnan.edu.cn">wu_xiaojun@jiangnan.edu.cn</a>)</li>
<li>Mr. Hefeng Yin, Jiangan University, China (<a href="mailto:yinhefeng@126.com">yinhefeng@126.com</a>)</li>
</ul>
<h3 id="programcommittee">Program Committee</h3>
<ul>
<li>Prof. <a href="https://www3.nd.edu/~kwb/">Kevin W. Bowyer</a>, University of Notre Dame, USA</li>
<li>Prof. <a href="https://www.surrey.ac.uk/cvssp/people/richard_bowden/">Richard Bowden</a>, University of Surrey, UK</li>
<li>Dr. <a href="http://eggerbernhard.ch/">Bernhard Egger</a>, University of Basel, Switzerland</li>
<li>Dr. <a href="https://www.microsoft.com/en-us/research/people/yag/">Yandong Guo</a>, Microsoft Research, USA</li>
<li>Dr. <a href="http://personal.ee.surrey.ac.uk/Personal/S.Hadfield/biography.html">Simon Hadfield</a>, University of Surrey, UK</li>
<li>Prof. <a href="http://www.cbl.uh.edu/">Ioannis A. Kakadiaris</a>, University of Houston, USA</li>
<li>Dr. <a href="http://www.cse.msu.edu/~liuxm/">Xiaoming Liu</a>, Michigan State University, USA</li>
<li>Dr. <a href="https://www-users.cs.york.ac.uk/wsmith/">Will Smith</a>, University of York, UK</li>
<li>Dr. Xiaoning Song, Jiangnan University, China</li>
<li>Prof. <a href="http://www.yongxu.org/">Yong Xu</a>, Harbin Institute of Technology, China</li>
<li>Prof. <a href="http://www.patternrecognition.cn/~jian/">Jian Yang</a>, Nanjing University of Science and Technology, China</li>
</ul>
<h3 id="acknowledgement">Acknowledgement</h3>
<p>The workshop has been supported in part by the EPSRC <a href="http://www.facer2vm.org">FACER2VM</a> project.</p>
</div>