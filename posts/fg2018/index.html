<!doctype html><html lang=en itemscope itemtype=https://schema.org/WebSite><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>FACER2VM | IEEE FG 2018 Workshop on Dense 3D Reconstruction of 2D Face Images in the Wild</title><meta name=HandheldFriendly content=True><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://facer2vm.org/assets/css/style.c441c8f99776395c666cf386ed966f0c349094cc4975cf2bba661ecd079347ac.css integrity="sha256-xEHI&#43;Zd2OVxmbPOG7ZZvDDSQlMxJdc8rumYezQeTR6w="><script defer src=https://facer2vm.org/assets/js/script.81251190e7ffff14c3f67f353861d9a41a0c36dbea8c4edb01291af89c8c7f7a.js integrity="sha256-gSURkOf//xTD9n81OGHZpBoMNtvqjE7bASka&#43;JyMf3o="></script><link rel=canonical href=https://facer2vm.org/posts/fg2018/><meta property=og:title content="IEEE FG 2018 Workshop on Dense 3D Reconstruction of 2D Face Images in the Wild"><meta property=og:description content="News  01-FEB-2018: The evaluation script of the competition is available via https://github.com/patrikhuber/fg2018-competition. You have to use the script and the provided 7 3D facial landmarks for evaluation. You also should send us back the average error of the reconstructed 3D face for each 2D image."><meta property=og:type content=article><meta property=og:url content=https://facer2vm.org/posts/fg2018/><meta property=article:published_time content=2017-10-12T15:14:20&#43;00:00><meta property=article:modified_time content=2017-10-12T15:14:20&#43;00:00><meta name=twitter:card content=summary><meta name=twitter:title content="IEEE FG 2018 Workshop on Dense 3D Reconstruction of 2D Face Images in the Wild"><meta name=twitter:description content="News  01-FEB-2018: The evaluation script of the competition is available via https://github.com/patrikhuber/fg2018-competition. You have to use the script and the provided 7 3D facial landmarks for evaluation. You also should send us back the average error of the reconstructed 3D face for each 2D image."><meta name=referrer content=no-referrer-when-downgrade><meta name=generator content="Hugo 0.49.2"><meta name=robots content="index, follow"></head><body class="posts page production"><div id=scroll><main itemscope itemtype=http://schema.org/Event><article class=rows><header id=header class="main-column posts page"><h1 class=headline itemprop=name>IEEE FG 2018 Workshop on Dense 3D Reconstruction of 2D Face Images in the Wild</h1><div class=meta><span class=authors><span itemprop=contributor itemscope itemtype=http://schema.org/Person><a href=/people/zhenhua-feng/ itemprop=url><span itemprop=name>Zhenhua Feng</span></a></span></span>
<span class=divider>|</span>
<time class=date datetime=2017-10-12 itemprop=startDate>12 Oct 2017</time></div></header><div id=squealer></div><div id=main class="main-column posts page"><div class=content itemprop=description><div class=kg-card-markdown><h3 id=news>News</h3><ul><li>01-FEB-2018: The evaluation script of the competition is available via <a href=https://github.com/patrikhuber/fg2018-competition>https://github.com/patrikhuber/fg2018-competition</a>. You have to use the script and the provided 7 3D facial landmarks for evaluation. You also should send us back the average error of the reconstructed 3D face for each 2D image.</li><li>31-JAN-2018: The paper submission deadline has been extended to <strong>07-02-2018</strong>.</li><li>01-JAN-2018: Happy New Year! The test set for the workshop special session, i.e. the competition, is ready to download. The test set has 2000 2D images, including 656 high-quality and 1344 low-quality images, of more than one hundred subjects. The test set is a subset of the <a href=http://pics.stir.ac.uk/ESRC/index.htm>Stirling/ESRC 3D Face Database</a>. To obtain the test set, please follow the instruction of the database webpage. You have to download the <a href=http://pics.stir.ac.uk/ESRC/license%20agreement.pdf>license agreement</a>, sign and scan it and return to <a href=mailto:3dfacedb@gmail.com>3dfacedb@gmail.com</a> (and cc to <a href=mailto:z.feng@surrey.ac.uk>z.feng@surrey.ac.uk</a> please). Please note that any 2D or 3D face image/scan of the Stirling/ESRC 3D Dace Database shall not be used for your model training or hyper-parameter tuning. You have to use the provided validation set for this kind of purpose. The evaluation metric will be announced very soon.</li><li>05-DEC-2017: The validation set for the challenge is ready. To obtain the validation set, please download, print and sign the agreement file (<a href=http://personal.ee.surrey.ac.uk/Personal/Z.Feng/files/agreement.pdf>link</a>). Then you should send a scanned copy of the signed agreement file to Mr. Hefeng Yin (Email: <a href=mailto:yinhefeng@126.com>yinhefeng@126.com</a>, <a href=mailto:7141905017@vip.jiangnan.edu.cn>7141905017@vip.jiangnan.edu.cn</a>) to obtain the download link. The validation set has 161 2D in-the-wild images of 10 subjects and their ground-truth 3D face scans. The validation set is a part of the JNU-3D face dataset collected by the Jiangnan University. Please note that the 2D images and 3D face scans of the test set are in different format and resolution than the validation set. The test set and evaluation protocol will be released at the beginning of January 2018.</li><li>27-NOV-2017: Paper submission is open. Please go to the <strong><a href=https://cmt3.research.microsoft.com/DRFIW2018>CMT</a></strong> website for submission.</li><li>12-OCT-2017: The workshop webpage is online.</li></ul><h3 id=topicsofinterest>Topics of Interest</h3><p>3D face reconstruction from 2D images has become a very active topic in computer vision and computer graphics. The workshop aims to bring together the community to explore and address challenges in 3D face reconstruction of 2D in-the-wild images. Topics of interest include but are not limited to:</p><ul><li>3D face dataset and models</li><li>3D face reconstruction from single images and videos</li><li>3D-based face analysis and biometrics</li><li>Special session on 3D face reconstruction challenge (competition)</li><li>Other applications of 3D face models</li></ul><p>The workshop is co-located with the <a href=https://fg2018.cse.sc.edu/>13th IEEE Conference on Automatic Face &amp; Gesture Recognition</a> (FG2018), 15-19 May, 2018, Xian,China.</p><p><em><strong>Special Session</strong>: Competition</em><br>As a special session of the workshop, we will release a new benchmark dataset consisting of a number of identities. For each identity there are a number of 2D in-the-wild face images and a high-resolution 3D face scan ground-truth. Alongside the dataset we supply a standard protocol to allow independent comparison among different algorithms.<br>The submission deadline for all papers is <strong>07 FEB 2018</strong>.</p><h3 id=papersubmissionsystem>Paper Submission System</h3><p>The paper submission system is open. To prepare your manuscript, please follow the guideline of the IEEE FG 2018 main conference (<a href=https://fg2018.cse.sc.edu/submissions.html>link</a>). Note that, for the workshop, you have to submit your manuscript via the <strong><a href=https://cmt3.research.microsoft.com/DRFIW2018>CMT</a></strong> system, which is different from the submission system of the main FG conference.</p><h3 id=importantdates>Important Dates</h3><ul><li>01-12-2017 - Release of the Validation Set and Protocol (Competition)</li><li>01-01-2018 - Release of the Test Set (Competition)</li><li>07-02-2018 - Paper Submission Deadline</li><li>21-02-2018 - Decisions to Authors</li><li>01-03-2018 - Camera-Ready Deadline</li><li>15/19-05-2018 - Workshop</li></ul><h3 id=organisers>Organisers</h3><ul><li>Dr. <a href=https://sites.google.com/view/fengzhenhua>Zhenhua Feng</a>, University of Surrey, UK (<a href=mailto:z.feng@surrey.ac.uk>z.feng@surrey.ac.uk</a>)</li><li>Dr. <a href=http://www.patrikhuber.ch/>Patrik Huber</a>, University of Surrey, UK (<a href=mailto:p.huber@surrey.ac.uk>p.huber@surrey.ac.uk</a>)</li><li>Prof. <a href=https://www.stir.ac.uk/people/11587>Peter Hancock</a>, University of Stirling, UK (<a href=mailto:p.j.b.hancock@stir.ac.uk>p.j.b.hancock@stir.ac.uk</a>)</li><li>Prof. <a href=https://www.surrey.ac.uk/cvssp/people/josef_kittler/>Josef Kittler</a>, University of Surrey, UK (<a href=mailto:j.kittler@surrey.ac.uk>j.kittler@surrey.ac.uk</a>)</li><li>Dr. <a href=https://paulkoppen.com/>Paul Koppen</a>, University of Surrey, UK (<a href=mailto:p.koppen@surrey.ac.uk>p.koppen@surrey.ac.uk</a>)</li><li>Prof. <a href="https://scholar.google.com/citations?user=5IST34sAAAAJ&amp;hl=en">Xiaojun Wu</a>, Jiangnan University, China (<a href=mailto:wu_xiaojun@jiangnan.edu.cn>wu_xiaojun@jiangnan.edu.cn</a>)</li><li>Mr. Hefeng Yin, Jiangan University, China (<a href=mailto:yinhefeng@126.com>yinhefeng@126.com</a>)</li></ul><h3 id=programcommittee>Program Committee</h3><ul><li>Prof. <a href=https://www3.nd.edu/~kwb/>Kevin W. Bowyer</a>, University of Notre Dame, USA</li><li>Prof. <a href=https://www.surrey.ac.uk/cvssp/people/richard_bowden/>Richard Bowden</a>, University of Surrey, UK</li><li>Dr. <a href=http://eggerbernhard.ch/>Bernhard Egger</a>, University of Basel, Switzerland</li><li>Dr. <a href=https://www.microsoft.com/en-us/research/people/yag/>Yandong Guo</a>, Microsoft Research, USA</li><li>Dr. <a href=http://personal.ee.surrey.ac.uk/Personal/S.Hadfield/biography.html>Simon Hadfield</a>, University of Surrey, UK</li><li>Prof. <a href=http://www.cbl.uh.edu/>Ioannis A. Kakadiaris</a>, University of Houston, USA</li><li>Dr. <a href=http://www.cse.msu.edu/~liuxm/>Xiaoming Liu</a>, Michigan State University, USA</li><li>Dr. <a href=https://www-users.cs.york.ac.uk/wsmith/>Will Smith</a>, University of York, UK</li><li>Dr. Xiaoning Song, Jiangnan University, China</li><li>Prof. <a href=http://www.yongxu.org/>Yong Xu</a>, Harbin Institute of Technology, China</li><li>Prof. <a href=http://www.patternrecognition.cn/~jian/>Jian Yang</a>, Nanjing University of Science and Technology, China</li></ul><h3 id=acknowledgement>Acknowledgement</h3><p>The workshop has been supported in part by the EPSRC <a href=http://www.facer2vm.org>FACER2VM</a> project.</p></div></div><div class=info><div class=tags itemprop=keywords><a href=/tags/events/ class="tags tags-events button" rel=tag>Events</a></div><aside class=social itemprop=potentialAction itemscope itemtype=http://schema.org/CommunicateAction><a class=twitter href="https://twitter.com/share?text=IEEE%20FG%202018%20Workshop%20on%20Dense%203D%20Reconstruction%20of%202D%20Face%20Images%20in%20the%20Wild&url=https%3a%2f%2ffacer2vm.org%2fposts%2ffg2018%2f" target=_blank rel="noopener external nofollow noreferrer" referrerpolicy=no-referrer itemprop=target><i class="ic ic-twitter"></i><span class=hidden>Twitter</span></a>
<a class=facebook href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2ffacer2vm.org%2fposts%2ffg2018%2f" target=_blank rel="noopener external nofollow noreferrer" referrerpolicy=no-referrer itemprop=target><i class="ic ic-facebook"></i><span class=hidden>Facebook</span></a>
<a class=googleplus href="https://plus.google.com/share?url=https%3a%2f%2ffacer2vm.org%2fposts%2ffg2018%2f" target=_blank rel="noopener external nofollow noreferrer" referrerpolicy=no-referrer oitemprop=target><i class="ic ic-googleplus"></i><span class=hidden>Google+</span></a></aside></div><section class="people node columns summary" itemprop=item itemscope itemtype=http://schema.org/Person><figure class=profile-image><img src=/people/zhenhua-feng/profile_image.png alt="Zhenhua Feng" itemprop=image></figure><div class="rows bio"><header><h2 class=name><a href=/people/zhenhua-feng/ itemprop=url><span itemprop=honorificPrefix>Dr</span> <span itemprop=name>Zhenhua Feng</span></a></h2></header><blockquote class=about cite=/people/zhenhua-feng/ itemprop=description><p></blockquote><footer class=meta><ul><li class="location ic ic-location" itemprop=workLocation itemscope itemtype=http://schema.org/Place><span itemprop=address>Guildford, UK</span></ul></footer></div></section></div></article><nav id=links class="main-column posts page" itemscope itemtype=http://schema.org/SiteNavigationElement><ul class=nav itemscope itemtype=http://schema.org/ItemList><meta itemprop=numberOfItems content=15><li class=previous itemprop=itemListElement itemscope itemtype=http://schema.org/ListItem><meta itemprop=position content=2><a href=/posts/nhk-filming-at-cvssp/ rel=prev itemprop=url><section class=teaser><i class="ic ic-arrow-left"></i><h2 class=headline itemprop=name>NHK filming at CVSSP</h2><p class=excerpt itemprop=description>During the weekend, the Japanese TV Broadcaster NHK came to film an experiment in face matching, comparing the performance of our face recognition …</p></section></a><li class=next itemprop=itemListElement itemscope itemtype=http://schema.org/ListItem><meta itemprop=position content=0><a href=/posts/unconstrained-face-recognition-at-the-eab/ rel=next itemprop=url><section class=teaser><i class="ic ic-arrow-right"></i><h2 class=headline itemprop=name>Unconstrained Face Recognition at the EAB</h2><p class=excerpt itemprop=description>Prof Josef Kittler presented the EPSRC funded FACER2VM project on unconstrained face recognition at a joint meeting of the European Association for …</p></section></a></ul></nav></main><footer id=footer class=main-column><div class=credits><p class=copyright><a href=https://facer2vm.org/>© 2018 FACER2VM</a><p class=software>Published with <a href=//gohugo.io/>Hugo</a></div></footer><label class=hidden-close for=menu-switch></label></div><nav id=nav-buttons><a id=home-button class="nav-button button" href=https://facer2vm.org/><i class="ic ic-arrow-left"></i><span class=caption>Home</span></a>
<label id=menu-button class="nav-button button" for=menu-switch><i class="ic ic-menu"></i><span class=caption>Menu</span></label></nav><nav id=menu><p class=nav-label>Menu</p><ul><li class=nav- role=presentation><a href=/><span>Home</span></a><li class=nav-tags_publications role=presentation><a href=/tags/publications/><span>Publications</span></a><li class=nav-tags_invited-talks role=presentation><a href=/tags/invited-talks/><span>Invited talks</span></a><li class=nav-tags_in-the-news role=presentation><a href=/tags/in-the-news/><span>In the news</span></a><li class=nav-tags_software role=presentation><a href=/tags/software/><span>Software</span></a><li class=nav-about role=presentation><a href=/about/><span>About</span></a><li class=nav-management-structure role=presentation><a href=/management-structure/><span>Management structure</span></a><li class=nav-twitter><a href=//twitter.com/paul_koppen title=@Paul_Koppen><i class="ic ic-twitter"></i>Twitter</a><li class=nav-rss><a rel=alternate type=application/rss+xml href=https://facer2vm.org/index.xml><i class="ic ic-rss"></i>Subscribe</a></ul><label for=menu-switch class="close-button button">Close</label>
<input type=checkbox id=menu-switch value=open tabindex=-1 role=switch aria-checked=false></nav></body></html>